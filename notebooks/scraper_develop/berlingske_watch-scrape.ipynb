{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Berlingske forside scraper\n",
    "\n",
    "https://askubuntu.com/questions/396654/how-to-run-a-python-program-in-the-background-even-after-closing-the-terminal\n",
    "\n",
    "https://www.programiz.com/python-programming/json\n",
    "\n",
    "**Krav:**\n",
    "- Scanne Berlingskes forside med jævne mellemrum\n",
    "- Scanne forside-titler for indhold af bestemte nøgleord\n",
    "- Lagre information om artikler, der indeholder disse nøgleord\n",
    "- Tilføje til JSON\n",
    "- Kontrol for artikel allerede er lagret\n",
    "- Skal forvente timeouts\n",
    "- Evt. mailvarsel\n",
    "\n",
    "**Udfordring:**\n",
    "- Varsling hvis script kører død?\n",
    "\n",
    "**JSON-opbygning:**\n",
    "- entry[id]\n",
    "    - newspaper_name: ...\n",
    "    - newspaper_frontpage-url: ...\n",
    "    - frontpage_selector:\n",
    "    - keyword_search:[..., ..., ...]\n",
    "    - keyword_match: [...]\n",
    "    - article_title: ...\n",
    "    - article_link: ...\n",
    "    - article_html: ...\n",
    "    - article_datetime: ...\n",
    "    - encounter_datetime: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import scrapy\n",
    "import requests\n",
    "from scrapy import Selector\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "import re\n",
    "import time\n",
    "import uuid\n",
    "from random import randint\n",
    "from itertools import compress\n",
    "import json\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions\n",
    "def keyword_check(keywords, headline):\n",
    "    text = headline.css(\"div ::text\").getall()\n",
    "    text = ' '.join(text)\n",
    "    text = text.lower()\n",
    "    if any(word in text for word in keywords):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_headline(healine_sel):\n",
    "    headline = Selector(text = healine_sel.css(\"div:first-of-type\").get()).css(' ::text').getall()\n",
    "    headline = ''.join(headline)\n",
    "    headline = headline.lower()\n",
    "    headline = re.sub('\\n', ' ', headline)\n",
    "    headline = re.sub('\\s\\s', '', headline)\n",
    "    headline = re.sub('\\s\\s', '', headline)\n",
    "    return(headline)\n",
    "\n",
    "def get_articlelink(headline_sel):\n",
    "    link = headline_sel.css(\"::attr(href)\").get()\n",
    "    return(link)\n",
    "\n",
    "def get_article_info(headline):\n",
    "    \n",
    "    i = 3\n",
    "    \n",
    "    art_uuid = str(uuid.uuid4())\n",
    "    encounter_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M')\n",
    "    \n",
    "    frontpage_title = get_headline(headline)\n",
    "    article_link = headline.css(\"::attr(href)\").get()\n",
    "    \n",
    "    matches = list(compress(keywords, [keyword in frontpage_title for keyword in keywords]))\n",
    "    \n",
    "    while i > 0:\n",
    "        time_out = randint(2, 5)\n",
    "        time.sleep(time_out)\n",
    "        response_code = requests.get(article_link).status_code\n",
    "\n",
    "        if response_code == 200:     \n",
    "\n",
    "            info = dict()\n",
    "\n",
    "            html = requests.get(article_link).content\n",
    "            sel = Selector(text = html)\n",
    "\n",
    "            title_sel = \"title ::text\"\n",
    "            datetime_xpath = '//meta[contains(@property,\"article:published_time\")]/@content'\n",
    "\n",
    "            article_title = sel.css(title_sel).get()\n",
    "            article_datetime = sel.xpath(datetime_xpath).get()\n",
    "\n",
    "            info['uuid'] = art_uuid\n",
    "            info['article_accessed'] = 1\n",
    "            info['newspaper_name'] = 'Berlingske'\n",
    "            info['newspaper_frontpage_url'] = 'https://www.berlingske.dk/'\n",
    "            info['frontpage_selector'] = \"div.front.theme-berlingske\"\n",
    "            info['keywords_search'] = keywords\n",
    "            info['keywords_match'] = matches\n",
    "            info['article_title'] = article_title\n",
    "            info['frontpage_title'] = frontpage_title\n",
    "            info['article_link'] = article_link\n",
    "            info['article_datetime'] = article_datetime\n",
    "            info['encounter_datetime'] = encounter_time\n",
    "            return(info)\n",
    "        else:\n",
    "            i = i -1\n",
    "        \n",
    "        if i == 0:\n",
    "            \n",
    "            info = dict()\n",
    "            \n",
    "            info['uuid'] = art_uuid\n",
    "            info['article_accessed'] = 0\n",
    "            info['newspaper_name'] = 'Berlingske'\n",
    "            info['newspaper_frontpage_url'] = 'https://www.berlingske.dk/'\n",
    "            info['frontpage_selector'] = \"div.front.theme-berlingske\"\n",
    "            info['keywords_search'] = keywords\n",
    "            info['keywords_match'] = matches\n",
    "            info['article_title'] = ''\n",
    "            info['frontpage_title'] = frontpage_title\n",
    "            info['article_link'] = article_link\n",
    "            info['article_datetime'] = ''\n",
    "            info['encounter_datetime'] = encounter_time\n",
    "            return(info)\n",
    "\n",
    "def front_page_check(url, keywords, url_list):\n",
    "    #selector of main page\n",
    "    url = url\n",
    "    html = requests.get(url).content\n",
    "    sel = Selector(text = html)\n",
    "\n",
    "    #selector of top frontpage contet\n",
    "    front_sel = \"div.front.theme-berlingske\"\n",
    "    front_page = sel.css(front_sel)\n",
    "\n",
    "    #get headline selectors\n",
    "    headline_xpath = '//article/div/a[contains(@class,\"dre-item\")]'\n",
    "    headlines = front_page.xpath(headline_xpath)\n",
    "\n",
    "    #extract headlines based on keyword\n",
    "    headlines_ext = list()\n",
    "\n",
    "    for headline in headlines:\n",
    "        if keyword_check(keywords, headline) == True:\n",
    "            headlines_ext.append(headline)\n",
    "    \n",
    "    #get article info\n",
    "    articles = list()\n",
    "\n",
    "    for headline in headlines_ext:\n",
    "        link = get_articlelink(headline)\n",
    "        if not link in url_list:\n",
    "            art_info = get_article_info(headline)\n",
    "            articles.append(art_info)\n",
    "            url_list.append(link)\n",
    "    \n",
    "    return(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def headline_watch(keywords, datadir, main_url = 'https://www.berlingske.dk/'):\n",
    "    keywords = keywords\n",
    "    \n",
    "    urldir = datadir + \"urls/\"\n",
    "    \n",
    "    urllist_filename = \"berlingske_article_urls.txt\"\n",
    "    \n",
    "    data_filename = \"berlingske_articles.json\"\n",
    "    \n",
    "    url_list = []\n",
    "    \n",
    "    try:\n",
    "        with open(urldir + urllist_filename, 'r') as f:\n",
    "            for line in f:\n",
    "                url_list.append(line.strip())\n",
    "            f. close()\n",
    "    except IOError:\n",
    "        print(\"No existing url list. Creating new file {}\".format(urllist_filename))\n",
    "\n",
    "    i = 3\n",
    "    \n",
    "    while i > 0:\n",
    "        try:\n",
    "            response = requests.get(main_url)\n",
    "            break\n",
    "        except:\n",
    "            i = i - 1\n",
    "            time_int = random.uniform(0.1, 0.2) \n",
    "            time.sleep(time_int)\n",
    "            continue\n",
    "    \n",
    "    if i > 0: \n",
    "        if response.status_code == 200:\n",
    "            articles = front_page_check(url = main_url, keywords = keywords, url_list = url_list)\n",
    "\n",
    "            if len(articles) != 0:\n",
    "                with open(datadir + data_filename, 'a') as file:\n",
    "                    json.dump(articles, file)\n",
    "\n",
    "            for article in articles:\n",
    "                url_list.append(article['article_link'])\n",
    "            \n",
    "            url_list = list(set(url_list))\n",
    "\n",
    "            with open(urldir + urllist_filename, 'w') as f:\n",
    "                for url in url_list:\n",
    "                    f.write(url + \"\\n\")\n",
    "                f.close()\n",
    "            \n",
    "            print(\"Berlingske front page checked on {time}. {n} new articles found.\".format(time = datetime.datetime.now(), n = len(articles)))\n",
    "            return\n",
    "    else:\n",
    "        print(\"Error retrieving front page on {time}. Skipping...\".format(time = datetime.datetime.now()))\n",
    "        return       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berlingske front page checked on 2020-06-16 14:15:40.692362. 0 new articles found.\n"
     ]
    }
   ],
   "source": [
    "keywords = ['klima', 'miljø', 'klimalov', 'grøn', 'bæredygtig', 'fossil', 'olie']\n",
    "\n",
    "datadir = \"../data/\"\n",
    "\n",
    "headline_watch(keywords = keywords, datadir = datadir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
