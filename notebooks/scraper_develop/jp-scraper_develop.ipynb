{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import re\n",
    "import time\n",
    "import uuid\n",
    "import random\n",
    "from random import randint\n",
    "from itertools import compress\n",
    "import json\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\"DR\": {\"url\": \"https://www.dr.dk/nyheder/politik/\",\n",
    "                 \"heading_tag\": \"a\",\n",
    "                 \"heading_class_regex\": \"dre-teaser-title*\"}, \n",
    "          \"Politiken\": {\"url\": \"https://politiken.dk/indland/politik/\",\n",
    "                        \"heading_tag\":\"h2\",\n",
    "                        \"heading_class_regex\": \"article-intro__title headline*\"},\n",
    "          \"Berlingske\": {\"url\": \"https://www.berlingske.dk/nyheder/politik/\",\n",
    "                         \"heading_tag\": \"a\", \n",
    "                         \"heading_class_regex\": \"teaser__title-link\"},\n",
    "          \"TV2\": {\"url\": \"https://nyheder.tv2.dk/politik/\",\n",
    "                  \"heading_tag\": \"a\",\n",
    "                  \"heading_class_regex\": \"o-teaser_link\"},\n",
    "          \"EB\": {\"url\": \"https://ekstrabladet.dk/nyheder/politik/\",\n",
    "                 \"heading_tag\": \"a\",\n",
    "                 \"heading_class_regex\": \"card\"},\n",
    "          \"JP\": {\"url\": \"https://jyllands-posten.dk/politik/\",\n",
    "                 \"heading_tag\": \"a\",\n",
    "                 \"heading_class_regex\": \"article-teaser-heading\"}\n",
    "                }\n",
    "\n",
    "SOURCES = tuple(PARAMS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(PARAMS['JP']['url'])\n",
    "\n",
    "jp_html = response.content\n",
    "\n",
    "jpsoup = bs(jp_html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_soup = jpsoup.find('section', id = 'section_a_zone_3') # Sektion uden mest lÃ¦ste og top\n",
    "heading_soups = section_soup.find_all(PARAMS['JP']['heading_tag'], class_ = re.compile(PARAMS['JP']['heading_class_regex']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(soup):\n",
    "    try:\n",
    "        article_title = soup.title.get_text()\n",
    "    except:\n",
    "        article_title = \"\"\n",
    "        \n",
    "    return(article_title)\n",
    "\n",
    "\n",
    "def get_datetime(soup):\n",
    "    try:\n",
    "        article_datetime = soup.find(\"meta\", attrs={\"name\": \"article:published_time\"})['content']\n",
    "    except (TypeError, KeyError):\n",
    "        try:\n",
    "            article_datetime = soup.find(\"meta\", attrs={'property': re.compile('article:published_time')})['content']\n",
    "        except:\n",
    "            article_datetime = \"\"\n",
    "\n",
    "    return(article_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_jp(pagesoup, page_params, keywords = [r\".*\"]):\n",
    "    section_soup = pagesoup.find('section', id = 'section_a_zone_3')\n",
    "    \n",
    "    #get headline soups\n",
    "    headlines = section_soup.find_all(page_params['heading_tag'], class_=re.compile(page_params['heading_class_regex']))\n",
    "\n",
    "    #extract headlines based on keyword\n",
    "    headlines_ext = list()\n",
    "\n",
    "    for headline in headlines:\n",
    "        if keyword_check(keywords, headline) == True:\n",
    "            headlines_ext.append(headline)\n",
    "       \n",
    "    links = list()\n",
    "    for headline in headlines_ext:\n",
    "        try:\n",
    "            link = urljoin(\"https://jyllands-posten.dk/\", headline['href'])\n",
    "            links.append(link)\n",
    "        except:\n",
    "            continue\n",
    "    links = list(filter(None, links))\n",
    "    links = list(set(links))\n",
    "    \n",
    "    return(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_check(keywords, headline):\n",
    "    '''\n",
    "    Checks whether headline contains keywords.\n",
    "    '''\n",
    "    text = headline.get_text().lower()\n",
    "    if any(re.match(word, text) for word in keywords):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_article_info(link, keywords, source, source_url):\n",
    "    '''\n",
    "    Creates a dictionary of information from a headline.\n",
    "    '''\n",
    "    i = 3\n",
    "    \n",
    "    art_uuid = str(uuid.uuid4())\n",
    "    encounter_time = datetime.now().strftime('%Y-%m-%d %H:%M')\n",
    "    \n",
    "    while i > 0:\n",
    "        time_out = random.uniform(1, 2)\n",
    "        time.sleep(time_out)\n",
    "        req = requests.get(link, timeout = 5.0)\n",
    "        response_code = req.status_code\n",
    "\n",
    "        if response_code == 200:     \n",
    "\n",
    "            info = dict()\n",
    "\n",
    "            html = requests.get(link, timeout = 5.0).content\n",
    "            soup = bs(html, \"html.parser\")\n",
    "\n",
    "            article_title = get_title(soup)\n",
    "            article_datetime = get_datetime(soup)\n",
    "\n",
    "            matches = list(compress(keywords, [keyword in article_title.lower() for keyword in keywords]))\n",
    "\n",
    "            info['uuid'] = art_uuid\n",
    "            info['article_accessed'] = 1\n",
    "            info['newspaper_name'] = source\n",
    "            info['newspaper_frontpage_url'] = source_url\n",
    "            info['keywords_search'] = keywords\n",
    "            info['keywords_match'] = matches\n",
    "            info['article_title'] = article_title\n",
    "            info['article_link'] = link\n",
    "            info['article_datetime'] = article_datetime\n",
    "            info['encounter_datetime'] = encounter_time\n",
    "            info['article_source'] = str(bs(req.content, \"html.parser\"))\n",
    "            return(info)\n",
    "        else:\n",
    "            i = i -1\n",
    "        \n",
    "        if i == 0:\n",
    "            \n",
    "            info = dict()\n",
    "            \n",
    "            info['uuid'] = art_uuid\n",
    "            info['article_accessed'] = 0\n",
    "            info['newspaper_name'] = source\n",
    "            info['newspaper_frontpage_url'] = source_url\n",
    "            info['keywords_search'] = keywords\n",
    "            info['keywords_match'] = ''\n",
    "            info['article_title'] = ''\n",
    "            info['article_link'] = link\n",
    "            info['article_datetime'] = ''\n",
    "            info['encounter_datetime'] = encounter_time\n",
    "            info['article_source'] = ''\n",
    "            return(info)\n",
    "\n",
    "def front_page_check(source, keywords, url_list):\n",
    "    '''\n",
    "    Creates dictionary of headlines with various information.\n",
    "    '''    \n",
    "    #get parameters\n",
    "    if source not in SOURCES:\n",
    "        raise Exception(\"{source} is not a valid source. Valid sources are {sources}.\".format(source = source, sources = re.sub(r'\\(|\\)', '', str(SOURCES))))\n",
    "        \n",
    "    page_params = PARAMS[source]\n",
    "    \n",
    "    #selector of main page\n",
    "    url = page_params['url']\n",
    "    html = requests.get(url, timeout = 5.0).content\n",
    "    soup = bs(html, \"html.parser\")\n",
    "\n",
    "    #get headline soups\n",
    "    headlines = soup.find_all(page_params['heading_tag'], class_=re.compile(page_params['heading_class_regex']))\n",
    "\n",
    "    #extract headlines based on keyword\n",
    "    headlines_ext = list()\n",
    "\n",
    "    for headline in headlines:\n",
    "        if keyword_check(keywords, headline) == True:\n",
    "            headlines_ext.append(headline)\n",
    "\n",
    "    #get links from extracted headlines\n",
    "    if source == \"DR\":\n",
    "        links_ext = get_links_dr(headlines_ext)\n",
    "    elif source == \"Politiken\":\n",
    "        links_ext = get_links_pol(headlines_ext)\n",
    "    elif source == \"Berlingske\":\n",
    "        links_ext = get_links_ber(headlines_ext)\n",
    "    elif source == \"TV2\":\n",
    "        links_ext = get_links_tv2(headlines_ext)\n",
    "    elif source == \"EB\":\n",
    "        links_ext = get_links_eb(soup, page_params)\n",
    "    elif source == \"JP\":\n",
    "        links_ext = get_links_jp(soup, page_params)\n",
    "\n",
    "    #get article info\n",
    "    articles = []\n",
    "\n",
    "    for link in links_ext:\n",
    "        if not link in url_list:\n",
    "            art_info = get_article_info(link = link, keywords = keywords, source = source, source_url = page_params['url'])\n",
    "            articles.append(art_info)\n",
    "            url_list.append(link)\n",
    "            \n",
    "    return(articles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
