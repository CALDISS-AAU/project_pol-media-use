{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "\n",
    "# Selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException, WebDriverException, ElementNotInteractableException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "# Chromedriver options\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-extensions\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "# Dirs\n",
    "driver_path = os.path.join('C:/', 'chromedriver', 'chromedriver.exe')\n",
    "datapath = os.path.join('D:/', 'data',  'dk_news', 'articles_20210406')\n",
    "outpath = os.path.join(datapath, 'articles_with-text_20210406.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "\n",
    "datafiles = [os.path.join(datapath,f) for f in os.listdir(datapath) if os.path.isfile(os.path.join(datapath, f))]\n",
    "datafiles = [datafile for datafile in datafiles if re.match(r'.*\\.json', datafile)]\n",
    "\n",
    "data = list()\n",
    "\n",
    "for datafile in datafiles:\n",
    "    with open(datafile, 'r', encoding = 'utf-8') as f:\n",
    "        entries = json.load(f)\n",
    "        data = data + entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions\n",
    "\n",
    "def get_arttext_ber(link):\n",
    "    driver = webdriver.Chrome(executable_path = driver_path, options=chrome_options)\n",
    "\n",
    "    driver.get(link)\n",
    "    pageSource = driver.page_source.encode(\"utf-8\")\n",
    "    driver.quit()\n",
    "\n",
    "    soup = bs(pageSource, 'html.parser')\n",
    "    \n",
    "    if soup.find(class_ = \"paywall\"):\n",
    "        paywall = True\n",
    "        text = ''\n",
    "    else:\n",
    "        paywall = False\n",
    "        text = soup.find('div', id = \"articleBody\").get_text()\n",
    "    \n",
    "    return(paywall, text)\n",
    "\n",
    "\n",
    "def get_arttext_pol(link):\n",
    "    driver = webdriver.Chrome(executable_path = driver_path, options=chrome_options)\n",
    "\n",
    "    driver.get(link)\n",
    "    pageSource = driver.page_source.encode(\"utf-8\")\n",
    "    driver.quit()\n",
    "\n",
    "    soup = bs(pageSource, 'html.parser')\n",
    "    \n",
    "    if soup.find(class_ = \"stopsign\"):\n",
    "        paywall = True\n",
    "        text = ''\n",
    "    else:\n",
    "        paywall = False\n",
    "        text = soup.find('div', class_ = \"article__body\").get_text()\n",
    "    \n",
    "    return(paywall, text)\n",
    "\n",
    "\n",
    "def get_arttext_dr(link):\n",
    "    # Undg√• \"engagement\" links\n",
    "    \n",
    "    if \"nyheder/politik/\" not in link:\n",
    "        return(None, None)\n",
    "    \n",
    "    driver = webdriver.Chrome(executable_path = driver_path, options=chrome_options)\n",
    "\n",
    "    driver.get(link)\n",
    "    pageSource = driver.page_source.encode(\"utf-8\")\n",
    "    driver.quit()\n",
    "\n",
    "    soup = bs(pageSource, 'html.parser')\n",
    "    \n",
    "    paywall = False\n",
    "    text = soup.find('div', class_ = \"dre-article-body\").get_text()\n",
    "    \n",
    "    return(paywall, text)\n",
    "\n",
    "\n",
    "def get_arttext_eb(link):\n",
    "    \n",
    "    driver = webdriver.Chrome(executable_path = driver_path, options=chrome_options)\n",
    "\n",
    "    driver.get(link)\n",
    "    pageSource = driver.page_source.encode(\"utf-8\")\n",
    "    driver.quit()\n",
    "\n",
    "    soup = bs(pageSource, 'html.parser')\n",
    "    \n",
    "    if soup.find(class_ = re.compile(r'paywall')):\n",
    "        paywall = True\n",
    "        text = ''\n",
    "    else:\n",
    "        paywall = False\n",
    "        text = soup.find('div', class_ = \"article-bodytext\").get_text()\n",
    "    \n",
    "    return(paywall, text)\n",
    "\n",
    "\n",
    "def get_arttext_jp(link):\n",
    "    \n",
    "    driver = webdriver.Chrome(executable_path = driver_path, options=chrome_options)\n",
    "\n",
    "    driver.get(link)\n",
    "    time.sleep(0.5)\n",
    "    pageSource = driver.page_source.encode(\"utf-8\")\n",
    "    driver.quit()\n",
    "\n",
    "    soup = bs(pageSource, 'html.parser')\n",
    "    \n",
    "    paywall = False\n",
    "    text = soup.find('article-body').get_text()\n",
    "    \n",
    "    return(paywall, text)\n",
    "\n",
    "\n",
    "def get_arttext_tv2(link):\n",
    "\n",
    "    if \"nyheder.tv2\" not in link:\n",
    "        return(None, None)    \n",
    "    \n",
    "    driver = webdriver.Chrome(executable_path = driver_path, options=chrome_options)\n",
    "\n",
    "    driver.get(link)\n",
    "    time.sleep(0.5)\n",
    "    pageSource = driver.page_source.encode(\"utf-8\")\n",
    "    driver.quit()\n",
    "\n",
    "    soup = bs(pageSource, 'html.parser')\n",
    "    \n",
    "    paywall = False\n",
    "    text = soup.find(attrs = {'data-adobe-context': 'article-body'}).get_text()\n",
    "    \n",
    "    return(paywall, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|==================================================| 100.00 %\r"
     ]
    }
   ],
   "source": [
    "for c, entry in enumerate(data, start = 1):\n",
    "    if entry.get('newspaper_name' == 'TV2'):\n",
    "        entry['article_paywall'], entry['article_text'] = get_arttext_tv2(entry.get('article_link'))\n",
    "    elif entry.get('newspaper_name' == 'Berlingske'):\n",
    "        entry['article_paywall'], entry['article_text'] = get_arttext_ber(entry.get('article_link'))\n",
    "    elif entry.get('newspaper_name' == 'Politiken'):\n",
    "        entry['article_paywall'], entry['article_text'] = get_arttext_pol(entry.get('article_link'))\n",
    "    elif entry.get('newspaper_name' == 'DR'):\n",
    "        entry['article_paywall'], entry['article_text'] = get_arttext_dr(entry.get('article_link'))    \n",
    "    elif entry.get('newspaper_name' == 'EB'):\n",
    "        entry['article_paywall'], entry['article_text'] = get_arttext_eb(entry.get('article_link'))  \n",
    "    elif entry.get('newspaper_name' == 'JP'):\n",
    "        entry['article_paywall'], entry['article_text'] = get_arttext_jp(entry.get('article_link'))  \n",
    "    \n",
    "    progress = \"|{0}| {1:.2f} %\".format((\"=\"*int(c/len(data) * 50)).ljust(50), c/len(data) * 100)\n",
    "    print(progress, end = \"\\r\")\n",
    "    \n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outpath, 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
